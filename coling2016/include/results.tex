\vspace{-0.2cm}


In this section, we compare the predictive performances of the classifiers on the two other datasets -- AmazonJ and the cleaned BU2 dataset.
From Figs. \ref{Fig:amazonj-branches+KL} and \ref{Fig:BU2-branches+KL}, we observe that these two datasets have similar type of imbalance (quantitatively).
However, although noisy, the dataset from BU2 is well-formed in its data organization taxonomy than a supposedly navigational taxonomy present in AmazonJ dataset.

The average micro-precisions for the two datasets and the dataset from BU1 are shown in Table \ref*{Table:average-prediction}. 
\begin{wraptable}{l}{0.8\textwidth}
	\vspace{-0.7cm}
	\centering
	{\small{
	\begin{tabular}{c c c c c c} \\ \hline 
		Dataset & NB &	LogReg ElasticNet (OvA) &	LogReg L1 (OvO) &	GBT	& CNN w pretraining  \\ \hline
		AmazonJ & 49.01	& 69.39 &	66.65 &	67.17 &	72.66* \\  
		BU2 & 68.21	& 84.29	& 85.01	& 90.63*	& 88.67 \\  
		BU1 & 81.45	& 86.30	& 86.75	& 89.03*	& 89.12* \\  \hline
	\end{tabular}
	}}
	\vspace{-0.4cm}
	\caption{\small{Average micro precisions of different classifiers on 10\% test sets from the three evaluation datasets using the feature set shown in the last graph of Fig. \ref{Figure_BU2-gbt-feature-improvements} with meta-features whenever applicable. The superscript $*$ means statistically significant than all other numbers in the corresponding row.}}
	\label{Table:average-prediction}
	\vspace{-0.3cm}
\end{wraptable}
It is observed that for the BU datasets, where $\log(N/B)$ with $N$ being the total number of listings and $B$ being the total number of branches (classes) is relatively high -- $11.54$ for BU2 and $9.27$ for BU1, but only $6.02$ for AmazonJ -- performances of GBT and CNN with pretraining are comparable for most of the subtrees and are better with statistical significance than the other classifiers.

\begin{wrapfigure}{l}{0.65\textwidth}
	\vspace{-0.5cm}
	\centering
	\includegraphics[width=0.65\textwidth]{images/amazonj-WUC-predictions}
	\vspace{-0.4cm}
	\caption{Micro precisions for 10\% test set from AmazonJ dataset}
	\label{Figure_amazonj-WUC-predictions}
	\vspace{-0.4cm}
\end{wrapfigure}
The ``\textit{CDs and Vinyl}'' category has more than $500$ branches in the AmazonJ dataset proved to be the most difficult one for all the classifiers.
We will like to investigate on this category in future.
The ``\textit{Books}'' and ``\textit{Grocery}'' subtrees were extremely imbalanced.
The root node of these subtrees had almost all the listings while the other branches mostly had less than 10 listings. 
The classifiers got high precision since almost all of the branches were excluded from prediction.


\begin{wrapfigure}{r}{0.48\textwidth}
	\vspace{-0.8cm}
	\centering
	\includegraphics[width=0.48\textwidth]{images/BU2-WUC-predictions}
	\vspace{-0.4cm}
	\caption{Micro precisions for 10\% test set from cleaned BU2 dataset}
	\label{Figure_BU2-WUC-predictions}
	\vspace{-0.4cm}
\end{wrapfigure}
Overall, from both Figs. \ref{Figure_amazonj-WUC-predictions} and \ref{Figure_BU2-WUC-predictions}, we observe that Gradient Boosted Trees and Convolutional Neural Networks perform quite well even in extreme data imbalance.
GBTs need more parameter tuning per subtree category for datasets resembling AmazonJ.
However, in most e-commerce companies, product listing datasets tend to be noisy but follow general trends of BU1 and BU2 datasets. 


